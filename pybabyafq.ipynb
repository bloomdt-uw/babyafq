{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pybabyafq(subject, session):    \n",
    "    import os.path as op\n",
    "    import os\n",
    "    from dipy.data.fetcher import _make_fetcher\n",
    "    import nibabel as nib\n",
    "    import s3fs\n",
    "    import json\n",
    "    from AFQ import api\n",
    "    from AFQ.definitions.mapping import AffMap\n",
    "    from AFQ.definitions.mask import MaskFile\n",
    "\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    afq_home = op.join(op.expanduser('~'), 'AFQ_data')\n",
    "    os.makedirs(afq_home, exist_ok=True)\n",
    "    \n",
    "    ##########################################################################\n",
    "    # Step 0: Pediatric templates\n",
    "    ##########################################################################\n",
    "\n",
    "    print('Step 0: Fetching pediatric templates...', flush=True)\n",
    "\n",
    "    pediatric_fnames = [\n",
    "        \"ATR_roi1_L.nii.gz\", \"ATR_roi1_R.nii.gz\",\n",
    "        \"ATR_roi2_L.nii.gz\", \"ATR_roi2_R.nii.gz\",\n",
    "        \"ATR_roi3_L.nii.gz\", \"ATR_roi3_R.nii.gz\",\n",
    "        \"CGC_roi1_L.nii.gz\", \"CGC_roi1_R.nii.gz\",\n",
    "        \"CGC_roi2_L.nii.gz\", \"CGC_roi2_R.nii.gz\",\n",
    "        \"CGC_roi3_L.nii.gz\", \"CGC_roi3_R.nii.gz\",\n",
    "        \"CST_roi1_L.nii.gz\", \"CST_roi1_R.nii.gz\",\n",
    "        \"CST_roi2_L.nii.gz\", \"CST_roi2_R.nii.gz\",\n",
    "        \"FA_L.nii.gz\", \"FA_R.nii.gz\",\n",
    "        \"FP_L.nii.gz\", \"FP_R.nii.gz\",\n",
    "        \"HCC_roi1_L.nii.gz\", \"HCC_roi1_R.nii.gz\",\n",
    "        \"HCC_roi2_L.nii.gz\", \"HCC_roi2_R.nii.gz\",\n",
    "        \"IFO_roi1_L.nii.gz\", \"IFO_roi1_R.nii.gz\",\n",
    "        \"IFO_roi2_L.nii.gz\", \"IFO_roi2_R.nii.gz\",\n",
    "        \"IFO_roi3_L.nii.gz\", \"IFO_roi3_R.nii.gz\",\n",
    "        \"ILF_roi1_L.nii.gz\", \"ILF_roi1_R.nii.gz\",\n",
    "        \"ILF_roi2_L.nii.gz\", \"ILF_roi2_R.nii.gz\",\n",
    "        \"LH_Parietal.nii.gz\", \"RH_Parietal.nii.gz\",\n",
    "        \"MdLF_roi1_L.nii.gz\", \"MdLF_roi1_R.nii.gz\",\n",
    "        \"SLF_roi1_L.nii.gz\", \"SLF_roi1_R.nii.gz\",\n",
    "        \"SLF_roi2_L.nii.gz\", \"SLF_roi2_R.nii.gz\",\n",
    "        \"SLFt_roi2_L.nii.gz\", \"SLFt_roi2_R.nii.gz\",\n",
    "        \"SLFt_roi3_L.nii.gz\", \"SLFt_roi3_R.nii.gz\",\n",
    "        \"UNC_roi1_L.nii.gz\", \"UNC_roi1_R.nii.gz\",\n",
    "        \"UNC_roi2_L.nii.gz\", \"UNC_roi2_R.nii.gz\",\n",
    "        \"UNC_roi3_L.nii.gz\", \"UNC_roi3_R.nii.gz\",\n",
    "        \"VOF_box_L.nii.gz\", \"VOF_box_R.nii.gz\",\n",
    "        \"UNCNeo-withCerebellum-for-babyAFQ.nii.gz\",\n",
    "        \"UNCNeo_JHU_tracts_prob-for-babyAFQ.nii.gz\",\n",
    "        \"mid-saggital.nii.gz\"\n",
    "    ]\n",
    "\n",
    "    pediatric_md5_hashes = [\n",
    "        \"2efe0deb19ac9e175404bf0cb29d9dbd\", \"c2e07cd50699527bd7b0cbbe88703c56\",\n",
    "        \"76b36d8d6759df58131644281ed16bd2\", \"645102225bad33da30bafd41d24b3ab0\",\n",
    "        \"45ec94d42fdc9448afa6760c656920e9\", \"54e3cb1b8c242be279f1410d8bb3c383\",\n",
    "        \"1ee9f7e8b21ef8ceee81d5a7a178ef33\", \"4f11097f7ae317aa8d612511be79e2f1\",\n",
    "        \"1c4c0823c23b676d6d35004d93b9c695\", \"d4830d558cc8f707ebec912b32d197a5\",\n",
    "        \"c405e0dbd9a4091c77b3d1ad200229b4\", \"ec0aeccc6661d2ee5ed79259383cdcee\",\n",
    "        \"2802cd227b550f6e85df0fec1d515c29\", \"385addb999dc6d76957d2a35c4ee74bb\",\n",
    "        \"b79f01829bd95682faaf545c72b1d52c\", \"b79f01829bd95682faaf545c72b1d52c\",\n",
    "        \"e49ba370edca96734d9376f551d413db\", \"f59e9e69e06325198f70047cd63c3bdc\",\n",
    "        \"ae3bd2931f95adae0280a8f75cd3ca9b\", \"c409a0036b8c2dd4d03d11fbc6bfbdcd\",\n",
    "        \"c2597a474ea5ec9e3126c35fd238f6b2\", \"67af59c934147c9f9ff6e0b76c4cc6eb\",\n",
    "        \"72d0bbc0b6162e9291fdc450c103a1f0\", \"51f5041be63ad0ac10d1ac09e3bf1a8e\",\n",
    "        \"6200f5cdc1402dce46dedd505468b147\", \"83cb5bf6b9b1eda63c8643871e84a6d4\",\n",
    "        \"2a5d8d309b1256d6e48958e112201c2c\", \"ba24d0915fdff215a403480d0a7745c9\",\n",
    "        \"1001e833d1344274d543ffd02a66af80\", \"03e20c5eebcd4d439c4ffb36d26a10d9\",\n",
    "        \"6200f5cdc1402dce46dedd505468b147\", \"83cb5bf6b9b1eda63c8643871e84a6d4\",\n",
    "        \"a6ae325cce2dc4bb21b52ee4c6ca844f\", \"a96a31df8f96ccf1c5f871a18e4a2d72\",\n",
    "        \"65b7378ca85689a5f23b1b84a6ed78f0\", \"ce0d0ea696ef51c671aa118e11192e2d\",\n",
    "        \"ce4918086ca1e829698576bcf95db62b\", \"96168d2eff74ec2acf9065f499732526\",\n",
    "        \"6b20ba319d44472ec21d6ece937605bb\", \"26b1cf6ec8bd365dde42e3efe9beeac2\",\n",
    "        \"0b3ccf06564d973bfcfff9a87e74f8b5\", \"84f3426033a2b225b0920b2622864375\",\n",
    "        \"5351b3cb7efa9aa8e83e266342809ebe\", \"4e8a34aaba4e0f22a6149f38620dc39d\",\n",
    "        \"682c08f66e8c2cf9e4e60f5ce308c76c\", \"9077affd4f3a8a1d6b44678cde4b3bf4\",\n",
    "        \"5adf36f00669cc547d5eb978acf46266\", \"66a8002688ffdf3943722361da90ec6a\",\n",
    "        \"efb5ae138df92019541861f9aa6a4d57\", \"757ec61078b2e9f9a073871b3216ff7a\",\n",
    "        \"ff1e238c52a21f8cc5d44ac614d9627f\", \"cf16dd2767c6ab2d3fceb2890f6c3e41\",\n",
    "        \"6016621e244b60b9c69fd44b055e4a03\", \"fd495a2c94b6b13bfb4cd63e293d3fc0\",\n",
    "        \"bf81a23d80f55e5f1eb0c16717193105\",\n",
    "        \"6f8bf8f70216788d14d9a49a3c664b16\",\n",
    "        \"19df0297d6a2ac21da5e432645d63174\",\n",
    "    ]\n",
    "\n",
    "    pediatric_remote_fnames = [\n",
    "        \"24880625\", \"24880628\", \"24880631\", \"24880634\", \"24880637\", \"24880640\",\n",
    "        \"24880643\", \"24880646\", \"24880649\", \"24880652\", \"24880655\", \"24880661\",\n",
    "        \"24880664\", \"24880667\", \"24880670\", \"24880673\", \"24880676\", \"24880679\",\n",
    "        \"24880685\", \"24880688\", \"24880691\", \"24880694\", \"24880697\", \"24880700\",\n",
    "        \"24880703\", \"24880706\", \"24880712\", \"24880715\", \"24880718\", \"24880721\",\n",
    "        \"24880724\", \"24880727\", \"24880730\", \"24880733\", \"24880736\", \"24880748\",\n",
    "        \"24880739\", \"24880742\", \"24880754\", \"24880757\", \"24880760\", \"24880763\",\n",
    "        \"24880769\", \"24880772\", \"24880775\", \"24880778\", \"24880781\", \"24880787\",\n",
    "        \"24880790\", \"24880793\", \"24880796\", \"24880802\", \"24880805\", \"24880808\",\n",
    "        \"24880616\", \"24880613\", \"24986396\"\n",
    "    ]\n",
    "\n",
    "    template_home = op.join(afq_home, 'pediatric_templates')\n",
    "    os.makedirs(template_home, exist_ok=True)\n",
    "    \n",
    "    fetch_pediatric_templates = _make_fetcher(\n",
    "        'fetch_pediatric_templates',\n",
    "        template_home,\n",
    "        'https://ndownloader.figshare.com/files/',\n",
    "        pediatric_remote_fnames,\n",
    "        pediatric_fnames,\n",
    "        md5_list=pediatric_md5_hashes,\n",
    "        doc='Download pediatric templates'\n",
    "    )\n",
    "\n",
    "    def read_pediatric_templates():\n",
    "        \"\"\"\n",
    "        Load pediatric pyAFQ templates. \n",
    "        \n",
    "        Used to create pediatric `bundle_dict`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict : \n",
    "            keys = names of template ROIs, and \n",
    "            values = `Nifti1Image` from each of the ROI nifti files.\n",
    "        \"\"\"\n",
    "        files, folder = fetch_pediatric_templates()\n",
    "\n",
    "        print('Loading pediatric templates...', flush=True)\n",
    "        pediatric_templates = {}\n",
    "        for f in files:\n",
    "            img = nib.load(op.join(folder, f))\n",
    "            pediatric_templates[f.split('.')[0]] = img\n",
    "\n",
    "        # For the arcuate (AF/ARC), reuse the SLF ROIs\n",
    "        pediatric_templates['ARC_roi1_L'] = pediatric_templates['SLF_roi1_L']\n",
    "        pediatric_templates['ARC_roi1_R'] = pediatric_templates['SLF_roi1_R']\n",
    "        pediatric_templates['ARC_roi2_L'] = pediatric_templates['SLFt_roi2_L']\n",
    "        pediatric_templates['ARC_roi2_R'] = pediatric_templates['SLFt_roi2_R']\n",
    "        pediatric_templates['ARC_roi3_L'] = pediatric_templates['SLFt_roi3_L']\n",
    "        pediatric_templates['ARC_roi3_R'] = pediatric_templates['SLFt_roi3_R']\n",
    "\n",
    "        # For the middle longitudinal fasciculus (MdLF) reuse ILF ROI\n",
    "        pediatric_templates['MdLF_roi2_L'] = pediatric_templates['ILF_roi2_L']\n",
    "        pediatric_templates['MdLF_roi2_R'] = pediatric_templates['ILF_roi2_R']\n",
    "\n",
    "        return pediatric_templates\n",
    "\n",
    "\n",
    "    pediatric_templates = read_pediatric_templates()\n",
    "\n",
    "    ##########################################################################\n",
    "    # Step 1: Pediatric bundle specification\n",
    "    ##########################################################################\n",
    "\n",
    "    print('Step 1: Creating pediatric bundle specification...', flush=True)\n",
    "\n",
    "    pediatric_bundle_names = [\n",
    "        \"ARC\",  # 'Arcuate Fasciculus'\n",
    "        \"ATR\",  # 'Thalamic Radiation'\n",
    "        \"CGC\",  # 'Cingulum Cingulate'\n",
    "        \"CST\",  # 'Corticospinal'\n",
    "        \"FA\",   # 'Forceps Minor'\n",
    "        \"FP\",   # 'Forceps Major'\n",
    "        \"IFO\",  # 'Inferior Fronto-occipital'\n",
    "        \"ILF\",  # 'Inferior Longitudinal Fasciculus'\n",
    "        \"MdLF\", # 'Middle Longitudinal Fasciculus'\n",
    "        \"SLF\",  # 'Superior Longitudinal Fasciculus'\n",
    "        \"UNC\"   # 'Uncinate Fasciculus'\n",
    "    ]\n",
    "\n",
    "    def make_pediatric_bundle_dict(bundle_names=pediatric_bundle_names):\n",
    "        \"\"\"\n",
    "        Create pyAFQ bundle dictionary object for pediatric subjects.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        bundle_names : (optional) list of pediatric bundle names, used to\n",
    "        generate `bundle_dict`. by default all pediatric bundles are included.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dict : pyAFQ `bundle_dict`\n",
    "        \"\"\"\n",
    "        # pediatric probability maps\n",
    "        prob_map_order = [\"ATR_L\", \"ATR_R\", \"CST_L\", \"CST_R\", \"CGC_L\", \"CGC_R\",\n",
    "                          \"HCC_L\", \"HCC_R\", \"FP\", \"FA\", \"IFO_L\", \"IFO_R\", \"ILF_L\",\n",
    "                          \"ILF_R\", \"SLF_L\", \"SLF_R\", \"UNC_L\", \"UNC_R\",\n",
    "                          \"ARC_L\", \"ARC_R\", \"MdLF_L\", \"MdLF_R\"]\n",
    "\n",
    "        prob_maps = pediatric_templates['UNCNeo_JHU_tracts_prob-for-babyAFQ']\n",
    "        prob_map_data = prob_maps.get_fdata()\n",
    "\n",
    "        # pediatric bundle dict\n",
    "        pediatric_bundles = {}\n",
    "\n",
    "        # each bundles gets a digit identifier (to be stored in the tractogram)\n",
    "        uid = 1\n",
    "\n",
    "        for name in bundle_names:\n",
    "            # ROIs that cross the mid-line\n",
    "            if name in [\"FA\", \"FP\"]:\n",
    "                pediatric_bundles[name] = {\n",
    "                    'ROIs': [pediatric_templates[name + \"_L\"],\n",
    "                             pediatric_templates[name + \"_R\"],\n",
    "                             pediatric_templates[\"mid-saggital\"]],\n",
    "                    'rules': [True, True, True],\n",
    "                    'cross_midline': True,\n",
    "                    'prob_map': prob_map_data[...,\n",
    "                                              prob_map_order.index(name)],\n",
    "                    'uid': uid}\n",
    "                uid += 1\n",
    "            # SLF is a special case, because it has an exclusion ROI:\n",
    "            elif name == \"SLF\":\n",
    "                for hemi in ['_R', '_L']:\n",
    "                    pediatric_bundles[name + hemi] = {\n",
    "                        'ROIs': [pediatric_templates[name + '_roi1' + hemi],\n",
    "                                 pediatric_templates[name + '_roi2' + hemi],\n",
    "                                 pediatric_templates[\"SLFt_roi2\" + hemi]],\n",
    "                        'rules': [True, True, False],\n",
    "                        'cross_midline': False,\n",
    "                        'prob_map': prob_map_data[...,\n",
    "                                                  prob_map_order.index(name + hemi)],\n",
    "                        'uid': uid}\n",
    "                    uid += 1\n",
    "            # Third ROI for curvy tracts\n",
    "            elif name in [\"ARC\", \"ATR\", \"CGC\", \"IFO\", \"UNC\"]:\n",
    "                for hemi in ['_R', '_L']:\n",
    "                    pediatric_bundles[name + hemi] = {\n",
    "                        'ROIs': [pediatric_templates[name + '_roi1' + hemi],\n",
    "                                 pediatric_templates[name + '_roi2' + hemi],\n",
    "                                 pediatric_templates[name + '_roi3' + hemi]],\n",
    "                        'rules': [True, True, True],\n",
    "                        'cross_midline': False,\n",
    "                        'prob_map': prob_map_data[...,\n",
    "                                                  prob_map_order.index(name + hemi)],\n",
    "                        'uid': uid}\n",
    "                    uid += 1\n",
    "            elif name == \"MdLF\":\n",
    "                for hemi in ['_R', '_L']:\n",
    "                    pediatric_bundles[name + hemi] = {\n",
    "                        'ROIs': [pediatric_templates[name + '_roi1' + hemi],\n",
    "                                 pediatric_templates[name + '_roi2' + hemi]],\n",
    "                        'rules': [True, True],\n",
    "                        'cross_midline': False,\n",
    "                        # reuse probability map from ILF\n",
    "                        'prob_map': prob_map_data[...,\n",
    "                                                  prob_map_order.index(\"ILF\" + hemi)],\n",
    "                        'uid': uid}\n",
    "                    uid += 1\n",
    "            # Default: two ROIs within hemisphere\n",
    "            else:\n",
    "                for hemi in ['_R', '_L']:\n",
    "                    pediatric_bundles[name + hemi] = {\n",
    "                        'ROIs': [pediatric_templates[name + '_roi1' + hemi],\n",
    "                                 pediatric_templates[name + '_roi2' + hemi]],\n",
    "                        'rules': [True, True],\n",
    "                        'cross_midline': False,\n",
    "                        'prob_map': prob_map_data[...,\n",
    "                                                  prob_map_order.index(name + hemi)],\n",
    "                        'uid': uid}\n",
    "                    uid += 1\n",
    "\n",
    "        return pediatric_bundles\n",
    "\n",
    "    ##########################################################################\n",
    "    # Step 2: Retrieve mrtrix derivatives\n",
    "    ##########################################################################\n",
    "    print('Step 2: Downloading mrtrix derivatives...', flush=True)\n",
    "    \n",
    "    dhcp_home = op.join(afq_home, 'dhcp')\n",
    "    os.makedirs(dhcp_home, exist_ok=True)\n",
    "    \n",
    "    dataset_description = {\n",
    "        \"Name\" : \"dHCP\",\n",
    "        \"BIDSVersion\" : \"1.4.0\"\n",
    "    }\n",
    "\n",
    "    with open(op.join(dhcp_home, 'dataset_description.json'), 'w') as f:\n",
    "        json.dump(dataset_description, f)\n",
    "    \n",
    "    mrtrix_derivatives = op.join(dhcp_home, 'derivatives', 'mrtrix')\n",
    "    os.makedirs(mrtrix_derivatives, exist_ok=True)\n",
    "    \n",
    "    fs.get(\n",
    "        f'dhcp-afq/mrtrix/dataset_description.json',\n",
    "        op.join(mrtrix_derivatives, 'dataset_description.json')\n",
    "    )\n",
    "    \n",
    "    subject_session_dir = op.join(mrtrix_derivatives, f'sub-{subject}', f'ses-{session}')\n",
    "    os.makedirs(subject_session_dir, exist_ok=True)\n",
    "    \n",
    "    files = [\n",
    "        f'sub-{subject}_ses-{session}_desc-csd_tractography.tck',\n",
    "        f'sub-{subject}_ses-{session}_desc-preproc_resliced_aligned_dwi.bval',\n",
    "        f'sub-{subject}_ses-{session}_desc-preproc_resliced_aligned_dwi.bvec',\n",
    "        f'sub-{subject}_ses-{session}_desc-preproc_resliced_aligned_dwi.nii.gz',\n",
    "        f'sub-{subject}_ses-{session}_desc-preproc_space_dwi_resliced_aligned_brainmask.nii.gz'\n",
    "    ]\n",
    "    \n",
    "    for file in files:\n",
    "        fs.get(\n",
    "            f'dhcp-afq/mrtrix/sub-{subject}/ses-{session}/{file}',\n",
    "            op.join(subject_session_dir, file)\n",
    "        )\n",
    "\n",
    "    ##########################################################################\n",
    "    # Step 3: DTI\n",
    "    ##########################################################################\n",
    "    print('Step 3: DTI', flush=True)\n",
    "    \n",
    "    myafq_dti = api.AFQ(\n",
    "        bids_path=dhcp_home,\n",
    "        dmriprep='dHCP neonatal MRtrix pipeline',\n",
    "        custom_tractography_bids_filters={'scope':'dHCP neonatal MRtrix pipeline', 'suffix':'tractography'},\n",
    "        reg_template=pediatric_templates['UNCNeo-withCerebellum-for-babyAFQ'],\n",
    "        reg_subject='b0',\n",
    "        mapping=AffMap(),\n",
    "        brain_mask=MaskFile('brainmask'),\n",
    "        max_bval=1000,\n",
    "        bundle_info=make_pediatric_bundle_dict(),\n",
    "        segmentation_params={'filter_by_endpoints': False, 'dist_to_waypoint': 0.75},\n",
    "        clean_params={'distance_threshold': 3}\n",
    "    )\n",
    "\n",
    "    # export all AFQ artifacts\n",
    "    myafq_dti.export_all()\n",
    "\n",
    "    ##########################################################################\n",
    "    # Step 4: DKI\n",
    "    ##########################################################################\n",
    "\n",
    "    print('Step 4: DKI', flush=True)\n",
    "    myafq_dki = api.AFQ(\n",
    "        bids_path=dhcp_home,\n",
    "        dmriprep='dHCP neonatal MRtrix pipeline',\n",
    "        custom_tractography_bids_filters={'scope':'dHCP neonatal MRtrix pipeline', 'suffix':'tractography'},\n",
    "        reg_template=pediatric_templates['UNCNeo-withCerebellum-for-babyAFQ'],\n",
    "        reg_subject='b0',\n",
    "        mapping=AffMap(),\n",
    "        brain_mask=MaskFile('brainmask'),\n",
    "        bundle_info=make_pediatric_bundle_dict(),\n",
    "        segmentation_params={'filter_by_endpoints': False, 'dist_to_waypoint': 0.75},\n",
    "        clean_params={'distance_threshold': 3}\n",
    "    )\n",
    "    \n",
    "    myafq_dki.dki_fa[0]\n",
    "    myafq_dki.dki_md[0]\n",
    "    myafq_dki.dki_mk[0]\n",
    "\n",
    "    ##########################################################################\n",
    "    # Step 5: Tract Profiles\n",
    "    ##########################################################################\n",
    "\n",
    "    print('Step 5: Tract Profiles', flush=True)\n",
    "    \n",
    "    # delete tract profile file prior to running\n",
    "    row = myafq_dki.data_frame.iloc[0]\n",
    "    tract_profiles_fname = myafq_dki._get_fname(row, '_profiles.csv')\n",
    "    os.remove(tract_profiles_fname)\n",
    "    \n",
    "    myafq_all = api.AFQ(\n",
    "        bids_path=dhcp_home,\n",
    "        dmriprep='dHCP neonatal MRtrix pipeline',\n",
    "        custom_tractography_bids_filters={'scope':'dHCP neonatal MRtrix pipeline', 'suffix':'tractography'},\n",
    "        reg_template=pediatric_templates['UNCNeo-withCerebellum-for-babyAFQ'],\n",
    "        reg_subject='b0',\n",
    "        scalars=[\"dti_fa\", \"dti_md\", \"dki_fa\", \"dki_md\", \"dki_mk\"],\n",
    "        mapping=AffMap(),\n",
    "        brain_mask=MaskFile('brainmask'),\n",
    "        bundle_info=make_pediatric_bundle_dict(),\n",
    "        segmentation_params={'filter_by_endpoints': False, 'dist_to_waypoint': 0.75},\n",
    "        clean_params={'distance_threshold': 3}\n",
    "    )\n",
    "    \n",
    "    # regenerate tract profiles for all scalars\n",
    "    myafq_all.tract_profiles\n",
    "\n",
    "    ##########################################################################\n",
    "    # Step 6: Upload pybabyafq derivatives\n",
    "    ##########################################################################\n",
    "    \n",
    "    print('Step 6: Uploading pyAFQ derivatives...', flush=True)\n",
    "    \n",
    "    afq_derivatives = op.join(dhcp_home, 'derivatives', 'afq')\n",
    "    \n",
    "    fs.put(afq_derivatives, 'dhcp-afq/afq/', recursive=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pybabyafq('CC00062XX05', '13801')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_session_pair(bucket_path):\n",
    "    \"\"\"\n",
    "    find subject session tuples from s3 file system\n",
    "    \n",
    "    not all subjects have corresponding session data, and some have multiple\n",
    "    \n",
    "    there is no metadata that lists theses pairs, so traverse the bucket\n",
    "    to identify\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bucket_path : string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of tuples containing subject_id and session_id\n",
    "    \"\"\"\n",
    "    import s3fs\n",
    "    fs = s3fs.S3FileSystem()\n",
    "\n",
    "    subject_session_pairs = []\n",
    "\n",
    "    for file in fs.ls(bucket_path):\n",
    "        if fs.isdir(file):\n",
    "            # directory bucket_path/sub-<subid>       \n",
    "            subject = file.split('/')[-1].split('-')[-1]\n",
    "            for file2 in fs.ls(file):\n",
    "                if fs.isdir(file2):\n",
    "                    # directory bucket_path/sub-<subid>/ses-<sesid>\n",
    "                    session = file2.split('/')[-1].split('-')[-1]\n",
    "                    subject_session_pairs.append((subject, session))\n",
    "    \n",
    "    return subject_session_pairs\n",
    "\n",
    "\n",
    "def check_anat_requirements(args):\n",
    "    \"\"\"\n",
    "    not all subjects have ribbon file. \n",
    "    this is common point of failure in pipeline.\n",
    "    remove subject from list.\n",
    "    \"\"\"\n",
    "    \n",
    "    import s3fs\n",
    "    fs = s3fs.S3FileSystem()\n",
    "\n",
    "    return [arg for arg in args if fs.exists(f'dhcp-afq/dhcp_anat_pipeline/sub-{arg[0]}/ses-{arg[1]}/anat/sub-{arg[0]}_ses-{arg[1]}_desc-ribbon_space-T2w_dseg.nii.gz')]\n",
    "\n",
    "\n",
    "# anat and dmri have different subject session pairs 558 and 490 respectively\n",
    "# will only want the intesection\n",
    "dhcp_anat_sub_ses = get_subject_session_pair('dhcp-afq/dhcp_anat_pipeline/')\n",
    "dhcp_dmri_sub_ses = get_subject_session_pair('dhcp-afq/dhcp_dmri_pipeline/')\n",
    "\n",
    "args = sorted(set(dhcp_anat_sub_ses) & set(dhcp_dmri_sub_ses))\n",
    "args = check_anat_requirements(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudknot as ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestamp = datetime.now().isoformat()[:-7].replace(':','-')[:-3]\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot = ck.Knot(\n",
    "    name='dhcp-' + timestamp,\n",
    "    func=pybabyafq,\n",
    "    base_image='python:3.7',\n",
    "    image_github_installs='https://github.com/yeatmanlab/pyAFQ.git',\n",
    "    pars_policies=('AmazonS3FullAccess',),\n",
    "    job_def_vcpus=8,\n",
    "    max_vcpus=8*len(args),\n",
    "    memory=64000,  # in MB\n",
    "    volume_size=250, # in GB\n",
    "    bid_percentage=105 # use spot instance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_futures = knot.map(args, starmap=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "knot.view_jobs()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "knot.jobs[0].status"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "knot.clobber(clobber_pars=True, clobber_repo=True, clobber_image=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
